{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MNIST Dataset Hacking.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPdoylm25P14fWN3AxyRcPK"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"e68pQyvKJC3A","colab_type":"text"},"source":["Beginner MNIST Neural Net Project\n","\n","Goal: To train a Deep Neural Network on the MNIST handwritten image dataset with above 98% accuracy.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oMRQxNMSJnez","colab_type":"text"},"source":["## Load a recent version of TensorFlow"]},{"cell_type":"code","metadata":{"id":"gBVvNiqVJO1s","colab_type":"code","colab":{}},"source":["# Install TensorFlow using Colab's tensorflow_version command\n","try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XCrFfQioJ1bE","colab_type":"text"},"source":["## Import Libraries\n","\n","Import TensorFlow, Numpy, Matplotlib libraries.\n","\n","Also import the TensorFlow datasets library so we can use the MNIST dataset.\n"]},{"cell_type":"code","metadata":{"id":"hHs3FLKEJ0bq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":33},"outputId":"18bfe4f0-e8d9-4afb-ab9a-e70a0303cf48","executionInfo":{"status":"ok","timestamp":1591342294844,"user_tz":-480,"elapsed":1053,"user":{"displayName":"Art Anderson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7RA4OjzdefhX0FWwH91atg6Qn0ZpTu_vYnjLbVg=s64","userId":"14519728763762088642"}}},"source":["# Import libraries\n","import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import matplotlib as plt\n","\n","# Import TensorFlow Datasets\n","import tensorflow_datasets as tfds\n","tfds.disable_progress_bar()\n","\n","# Print version of TensorFlow currently imported\n","print(tf.__version__)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["2.2.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dMdlbWCMKDjZ","colab_type":"text"},"source":["Problem: Human handwriting is imperfect compared to computerized text. Train a model to correctly predict the English characters within a set of handwritten images."]},{"cell_type":"markdown","metadata":{"id":"LRZkw8-vRKgd","colab_type":"text"},"source":["## Load MNIST\n","Load with the following arguments:\n","\n","\n","*   shuffle_files: The MNIST data is only stored in a single file, but for larger datasets with multiple files on disk, it's good practice to shuffle them when training.\n","*   as_supervised: Returns tuple (img, label) instead of dict {'image': img, 'label': label}"]},{"cell_type":"code","metadata":{"id":"jxXN18MASFgD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":33},"outputId":"c7552209-eaf0-4f95-8624-c4078cc67158","executionInfo":{"status":"ok","timestamp":1591341562717,"user_tz":-480,"elapsed":3329,"user":{"displayName":"Art Anderson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7RA4OjzdefhX0FWwH91atg6Qn0ZpTu_vYnjLbVg=s64","userId":"14519728763762088642"}}},"source":["# Load the mnist dataset using tdfs\n","(ds_train, ds_test), ds_info = tfds.load(\n","    'mnist',\n","    split=['train', 'test'],\n","    shuffle_files=True,\n","    as_supervised=True,\n","    with_info=True,\n","    try_gcs=True\n",")\n","\n","# Output one example to verify dataset has been imported\n","print(ds_train)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["<DatasetV1Adapter shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WpNAiSa8NoqF","colab_type":"text"},"source":["## Next up we build the training pipeline.\n","\n","We apply the following transormations to the dataset.\n","\n","\n","*   ds.map: TFDS provides the images as tf.uint8 encoding, while the model expects tf.float32, so normalize the images to float32.\n","*   ds.cache As the dataset fits into memory, cache it before shuffling for better performance. Note: Random transformations should be applied after caching.\n","*   ds.shuffle: For true randomness, set the shuffle buffer to the full size of the dataset.\n","Note: For bigger datasets which do not fit in memory, a standard value is 1000 if your system allows it.\n","*   ds.batch: Batch after shuffling to get unique batches at each epoch.\n","*   ds.prefetch: It is good practice to end the pipeline by prefetching for performance.\n","\n"]},{"cell_type":"code","metadata":{"id":"dzn_WLegNrYo","colab_type":"code","colab":{}},"source":["def normalize_img(image, label):\n","  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n","  return tf.cast(image, tf.float32) / 255., label\n","\n","ds_train = ds_train.map(\n","    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","ds_train = ds_train.cache()\n","ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n","ds_train = ds_train.batch(128)\n","ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"14VBsldHQAaG","colab_type":"text"},"source":["## Build evaluation pipeline\n","The Testing pipeline is similar to the training pipeline, with small tweaks:\n","\n","\n","*   There is no ds.shuffle() call\n","*   Caching is done after batching (as batches can be the same between epoch)\n"]},{"cell_type":"code","metadata":{"id":"9ZTAAsEbQilM","colab_type":"code","colab":{}},"source":["ds_test = ds_test.map(\n","    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","ds_test = ds_test.batch(128)\n","ds_test = ds_test.cache()\n","ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m13vl58IYODA","colab_type":"text"},"source":["# Define the plotting curve\n","This will be used to graph model accuracy"]},{"cell_type":"code","metadata":{"id":"NxpgrsSiYU34","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":33},"outputId":"b0b8d2d1-6eb1-4a0f-c9cd-46c516bc1701","executionInfo":{"status":"ok","timestamp":1591342095592,"user_tz":-480,"elapsed":1112,"user":{"displayName":"Art Anderson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7RA4OjzdefhX0FWwH91atg6Qn0ZpTu_vYnjLbVg=s64","userId":"14519728763762088642"}}},"source":["#@title Define the plotting function\n","def plot_curve(epochs, hist, list_of_metrics):\n","  \"\"\"Plot a curve of one or more classification metrics vs. epoch.\"\"\"  \n","  # list_of_metrics should be one of the names shown in:\n","  # https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#define_the_model_and_metrics  \n","\n","  plt.figure()\n","  plt.xlabel(\"Epoch\")\n","  plt.ylabel(\"Value\")\n","\n","  for m in list_of_metrics:\n","    x = hist[m]\n","    plt.plot(epochs[1:], x[1:], label=m)\n","\n","  plt.legend()\n","\n","print(\"Loaded the plot_curve function.\")"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Loaded the plot_curve function.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dgku72p8QkDr","colab_type":"text"},"source":["## Build and train the model\n","\n","Input the training pipeline built above into Keras and define model parameters"]},{"cell_type":"code","metadata":{"id":"Okb2d-XuR-CJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":351},"outputId":"9a197a07-a59f-4f8d-8f91-d9a6e455ce3d","executionInfo":{"status":"ok","timestamp":1591342859820,"user_tz":-480,"elapsed":19615,"user":{"displayName":"Art Anderson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7RA4OjzdefhX0FWwH91atg6Qn0ZpTu_vYnjLbVg=s64","userId":"14519728763762088642"}}},"source":["# These variables are the hyperparameters for the model.\n","learning_rate = 0.001\n","epochs = 10\n","batch_size = 4000\n","\n","\n","def create_model(learning_rate):\n","\n","  # Use an input shape of 28x28 because the images in MNIST are 28x28 pixels\n","  # reLU has been effective in most use cases including this one, add a relatively dense layer with 128 neurons\n","  # Use softmax activation function because we want a probability distribution\n","\n","  model = tf.keras.models.Sequential([\n","    tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n","    tf.keras.layers.Dense(128,activation='relu'),\n","    tf.keras.layers.Dense(10, activation='softmax')\n","  ])\n","\n","  # Compile model with SCC loss function, Adam optimizer and Accuracy metric\n","  # SCC is used when you have two or more label classes\n","  # Adam optimizer uses Stochastic Gradient Descent and is efficient when dealing with large datasets\n","  # Accuracy metric is used to calculate how often predictions from the model are equal to the real labels\n","\n","  model.compile(\n","      optimizer=tf.keras.optimizers.Adam(learning_rate),\n","      loss='sparse_categorical_crossentropy',\n","      metrics=['accuracy'],\n","  )\n","\n","  return model\n","\n","def train_model(model, epochs, batch_size):\n","\n","\n","  # Fit the model using the training split, then validate using the test split\n","  history = model.fit(\n","      ds_train,\n","      batch_size=batch_size,\n","      epochs=epochs,\n","      validation_data=ds_test\n","  )\n","\n","  # To track the progress of training, take a snapshot of the model metrics at each epoch and add it to a dataframe.\n","  epochs = history.epoch\n","  hist = pd.DataFrame(history.history)\n","\n","  # return epochs, hist \n","\n"],"execution_count":35,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","469/469 [==============================] - 2s 4ms/step - loss: 0.3564 - accuracy: 0.9030 - val_loss: 0.1877 - val_accuracy: 0.9457\n","Epoch 2/10\n","469/469 [==============================] - 2s 4ms/step - loss: 0.1632 - accuracy: 0.9534 - val_loss: 0.1359 - val_accuracy: 0.9614\n","Epoch 3/10\n","469/469 [==============================] - 2s 4ms/step - loss: 0.1174 - accuracy: 0.9664 - val_loss: 0.1086 - val_accuracy: 0.9686\n","Epoch 4/10\n","469/469 [==============================] - 2s 4ms/step - loss: 0.0921 - accuracy: 0.9736 - val_loss: 0.1027 - val_accuracy: 0.9697\n","Epoch 5/10\n","469/469 [==============================] - 2s 4ms/step - loss: 0.0738 - accuracy: 0.9788 - val_loss: 0.0873 - val_accuracy: 0.9745\n","Epoch 6/10\n","469/469 [==============================] - 2s 4ms/step - loss: 0.0621 - accuracy: 0.9817 - val_loss: 0.0804 - val_accuracy: 0.9747\n","Epoch 7/10\n","469/469 [==============================] - 2s 4ms/step - loss: 0.0518 - accuracy: 0.9849 - val_loss: 0.0884 - val_accuracy: 0.9715\n","Epoch 8/10\n","469/469 [==============================] - 2s 4ms/step - loss: 0.0442 - accuracy: 0.9876 - val_loss: 0.0748 - val_accuracy: 0.9765\n","Epoch 9/10\n","469/469 [==============================] - 2s 4ms/step - loss: 0.0370 - accuracy: 0.9898 - val_loss: 0.0756 - val_accuracy: 0.9763\n","Epoch 10/10\n","469/469 [==============================] - 2s 4ms/step - loss: 0.0317 - accuracy: 0.9911 - val_loss: 0.0743 - val_accuracy: 0.9775\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fiWfRRqqaxDP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":228},"outputId":"ff5069e9-cbc8-4e41-ccab-5f843ecdd60f","executionInfo":{"status":"error","timestamp":1591342941925,"user_tz":-480,"elapsed":998,"user":{"displayName":"Art Anderson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7RA4OjzdefhX0FWwH91atg6Qn0ZpTu_vYnjLbVg=s64","userId":"14519728763762088642"}}},"source":["# Plot the collected metrics over time\n","list_of_metrics_to_plot = ['accuracy']\n","plot_curve(epochs, hist list_of_metrics_to_plot)\n","\n","\n","# model.evaluate(x=x_test_normalized, y=y_test, batch_size=batch_size)"],"execution_count":38,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-fd3786bc7121>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot the collected metrics over time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlist_of_metrics_to_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplot_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# list_of_metrics_to_plot)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: plot_curve() missing 1 required positional argument: 'list_of_metrics'"]}]}]}